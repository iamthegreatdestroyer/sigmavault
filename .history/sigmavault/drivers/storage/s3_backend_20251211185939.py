"""
Î£VAULT S3 Storage Backend

Amazon S3 (and compatible) storage backend implementation.
Supports any S3-compatible storage service including:
    - AWS S3
    - MinIO
    - Backblaze B2
    - DigitalOcean Spaces
    - Cloudflare R2

Features:
    - Async and sync operations
    - Multipart uploads for large files
    - Range reads for efficient partial access
    - Automatic retries with exponential backoff
    - Connection pooling
"""

import asyncio
import hashlib
import io
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterator, List, Optional, Tuple, Union

try:
    import boto3
    from botocore.config import Config as BotoConfig
    from botocore.exceptions import ClientError, BotoCoreError
    HAS_BOTO3 = True
except ImportError:
    HAS_BOTO3 = False

try:
    import aioboto3
    HAS_AIOBOTO3 = True
except ImportError:
    HAS_AIOBOTO3 = False

from .base import (
    StorageBackend,
    StorageCapabilities,
    StorageError,
    StorageReadError,
    StorageWriteError,
    StorageCapacityError,
    StorageNotFoundError,
)


@dataclass
class S3Config:
    """Configuration for S3 storage backend."""
    
    bucket: str
    key_prefix: str = ""
    region: str = "us-east-1"
    endpoint_url: Optional[str] = None  # For S3-compatible services
    access_key_id: Optional[str] = None
    secret_access_key: Optional[str] = None
    
    # Performance tuning
    max_pool_connections: int = 50
    connect_timeout: float = 5.0
    read_timeout: float = 60.0
    max_retries: int = 3
    
    # Multipart upload settings
    multipart_threshold: int = 8 * 1024 * 1024  # 8 MB
    multipart_chunk_size: int = 8 * 1024 * 1024  # 8 MB
    max_concurrency: int = 10
    
    # Storage class
    storage_class: str = "STANDARD"  # STANDARD, STANDARD_IA, GLACIER, etc.


class S3StorageBackend(StorageBackend):
    """
    Storage backend using Amazon S3 or compatible services.
    
    This backend stores scattered data as objects in an S3 bucket.
    It supports both the scattered write pattern (many small objects)
    and consolidated storage (single large object with range reads).
    
    Thread Safety:
        All operations are thread-safe via boto3's built-in handling.
    
    Example:
        >>> config = S3Config(bucket="my-vault-bucket")
        >>> backend = S3StorageBackend(config)
        >>> backend.write(0, b"Hello S3")
        8
        >>> backend.read(0, 8)
        b'Hello S3'
    
    Note:
        Requires boto3 package: pip install boto3
        For async support: pip install aioboto3
    """
    
    def __init__(
        self,
        config: S3Config,
        object_key: str = "vault.dat",
        create: bool = True,
    ):
        """
        Initialize S3 storage backend.
        
        Args:
            config: S3 configuration.
            object_key: Key for the main storage object.
            create: Whether to create the object if it doesn't exist.
        
        Raises:
            ImportError: If boto3 is not installed.
            StorageError: If bucket doesn't exist or isn't accessible.
        """
        if not HAS_BOTO3:
            raise ImportError(
                "S3StorageBackend requires boto3. "
                "Install with: pip install boto3"
            )
        
        super().__init__(enable_stats=True)
        
        self._config = config
        self._object_key = f"{config.key_prefix}{object_key}" if config.key_prefix else object_key
        self._lock = threading.RLock()
        
        # Initialize S3 client
        self._client = self._create_client()
        
        # Verify bucket access
        self._verify_bucket()
        
        # Get or create the storage object
        self._size: int = 0
        self._initialize_storage(create)
    
    def _create_client(self):
        """Create boto3 S3 client with configuration."""
        boto_config = BotoConfig(
            max_pool_connections=self._config.max_pool_connections,
            connect_timeout=self._config.connect_timeout,
            read_timeout=self._config.read_timeout,
            retries={
                'max_attempts': self._config.max_retries,
                'mode': 'adaptive'
            }
        )
        
        kwargs = {
            'service_name': 's3',
            'region_name': self._config.region,
            'config': boto_config,
        }
        
        if self._config.endpoint_url:
            kwargs['endpoint_url'] = self._config.endpoint_url
        
        if self._config.access_key_id and self._config.secret_access_key:
            kwargs['aws_access_key_id'] = self._config.access_key_id
            kwargs['aws_secret_access_key'] = self._config.secret_access_key
        
        return boto3.client(**kwargs)
    
    def _verify_bucket(self) -> None:
        """Verify bucket exists and is accessible."""
        try:
            self._client.head_bucket(Bucket=self._config.bucket)
        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code', '')
            if error_code == '404':
                raise StorageNotFoundError(f"Bucket not found: {self._config.bucket}")
            elif error_code == '403':
                raise StorageError(f"Access denied to bucket: {self._config.bucket}")
            else:
                raise StorageError(f"Error accessing bucket: {e}")
    
    def _initialize_storage(self, create: bool) -> None:
        """Initialize or verify storage object."""
        try:
            # Try to get object metadata
            response = self._client.head_object(
                Bucket=self._config.bucket,
                Key=self._object_key
            )
            self._size = response['ContentLength']
        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code', '')
            if error_code == '404':
                if create:
                    # Create empty object
                    self._client.put_object(
                        Bucket=self._config.bucket,
                        Key=self._object_key,
                        Body=b'',
                        StorageClass=self._config.storage_class
                    )
                    self._size = 0
                else:
                    raise StorageNotFoundError(
                        f"Storage object not found: {self._object_key}"
                    )
            else:
                raise StorageError(f"Error initializing storage: {e}")
    
    # ========================================================================
    # StorageBackend Interface Implementation
    # ========================================================================
    
    def read(self, offset: int, size: int) -> bytes:
        """
        Read bytes from S3 storage at given offset.
        
        Uses S3 range reads for efficient partial access.
        
        Args:
            offset: Byte offset from start.
            size: Number of bytes to read.
        
        Returns:
            Bytes read.
        
        Raises:
            StorageReadError: If read fails.
            ValueError: If offset is negative.
        """
        if offset < 0:
            raise ValueError(f"Offset cannot be negative: {offset}")
        
        if size <= 0:
            return b''
        
        # Check if offset is beyond object size
        if offset >= self._size:
            return b''
        
        # Adjust size if it would read past end
        actual_size = min(size, self._size - offset)
        
        try:
            # Use range read
            range_header = f"bytes={offset}-{offset + actual_size - 1}"
            response = self._client.get_object(
                Bucket=self._config.bucket,
                Key=self._object_key,
                Range=range_header
            )
            
            data = response['Body'].read()
            self._record_read(len(data))
            return data
            
        except ClientError as e:
            raise StorageReadError(f"S3 read failed at offset {offset}: {e}")
    
    def write(self, offset: int, data: bytes) -> int:
        """
        Write bytes to S3 storage at given offset.
        
        Note: S3 doesn't support in-place writes. This implementation
        uses a read-modify-write pattern for small updates, or
        multipart upload for appending to the end.
        
        Args:
            offset: Byte offset from start.
            data: Bytes to write.
        
        Returns:
            Number of bytes written.
        
        Raises:
            StorageWriteError: If write fails.
            ValueError: If offset is negative.
        """
        if offset < 0:
            raise ValueError(f"Offset cannot be negative: {offset}")
        
        if not data:
            return 0
        
        with self._lock:
            try:
                # For writes at end (append), we can use multipart upload
                if offset == self._size:
                    return self._append_data(data)
                
                # For writes within existing content or creating gaps,
                # we need to read-modify-write
                return self._write_with_modify(offset, data)
                
            except ClientError as e:
                raise StorageWriteError(f"S3 write failed at offset {offset}: {e}")
    
    def _append_data(self, data: bytes) -> int:
        """Append data to end of storage."""
        if len(data) < self._config.multipart_threshold:
            # Small append - read all, append, write back
            existing = b''
            if self._size > 0:
                response = self._client.get_object(
                    Bucket=self._config.bucket,
                    Key=self._object_key
                )
                existing = response['Body'].read()
            
            new_data = existing + data
            self._client.put_object(
                Bucket=self._config.bucket,
                Key=self._object_key,
                Body=new_data,
                StorageClass=self._config.storage_class
            )
            self._size = len(new_data)
        else:
            # Large append - use multipart upload
            self._multipart_append(data)
        
        self._record_write(len(data))
        return len(data)
    
    def _write_with_modify(self, offset: int, data: bytes) -> int:
        """Write data at offset using read-modify-write."""
        # Read existing content
        existing = b''
        if self._size > 0:
            response = self._client.get_object(
                Bucket=self._config.bucket,
                Key=self._object_key
            )
            existing = response['Body'].read()
        
        # Pad with zeros if offset is beyond current size
        if offset > len(existing):
            existing = existing + b'\x00' * (offset - len(existing))
        
        # Merge data
        new_data = existing[:offset] + data + existing[offset + len(data):]
        
        # Write back
        self._client.put_object(
            Bucket=self._config.bucket,
            Key=self._object_key,
            Body=new_data,
            StorageClass=self._config.storage_class
        )
        
        self._size = len(new_data)
        self._record_write(len(data))
        return len(data)
    
    def _multipart_append(self, data: bytes) -> None:
        """Append data using multipart upload."""
        # This is a simplified implementation
        # For production, use boto3's TransferManager
        
        # Read existing content
        existing = b''
        if self._size > 0:
            response = self._client.get_object(
                Bucket=self._config.bucket,
                Key=self._object_key
            )
            existing = response['Body'].read()
        
        # Start multipart upload
        mpu = self._client.create_multipart_upload(
            Bucket=self._config.bucket,
            Key=self._object_key,
            StorageClass=self._config.storage_class
        )
        upload_id = mpu['UploadId']
        
        try:
            parts = []
            all_data = existing + data
            part_number = 1
            
            # Upload in chunks
            for i in range(0, len(all_data), self._config.multipart_chunk_size):
                chunk = all_data[i:i + self._config.multipart_chunk_size]
                
                response = self._client.upload_part(
                    Bucket=self._config.bucket,
                    Key=self._object_key,
                    UploadId=upload_id,
                    PartNumber=part_number,
                    Body=chunk
                )
                
                parts.append({
                    'PartNumber': part_number,
                    'ETag': response['ETag']
                })
                part_number += 1
            
            # Complete multipart upload
            self._client.complete_multipart_upload(
                Bucket=self._config.bucket,
                Key=self._object_key,
                UploadId=upload_id,
                MultipartUpload={'Parts': parts}
            )
            
            self._size = len(all_data)
            
        except Exception as e:
            # Abort failed upload
            self._client.abort_multipart_upload(
                Bucket=self._config.bucket,
                Key=self._object_key,
                UploadId=upload_id
            )
            raise
    
    def size(self) -> int:
        """Return total size of S3 storage object."""
        return self._size
    
    def sync(self) -> None:
        """
        Sync is a no-op for S3 as writes are immediately persisted.
        """
        pass
    
    @property
    def capabilities(self) -> StorageCapabilities:
        """Return capabilities of S3 storage backend."""
        return (
            StorageCapabilities.RANGE_READ |
            StorageCapabilities.CONCURRENT |
            StorageCapabilities.PERSISTENT |
            StorageCapabilities.SEEKABLE
        )
    
    # ========================================================================
    # Optional Methods
    # ========================================================================
    
    def truncate(self, size: int) -> None:
        """
        Truncate the storage object.
        
        Args:
            size: New size in bytes.
        """
        with self._lock:
            if size == 0:
                # Empty the object
                self._client.put_object(
                    Bucket=self._config.bucket,
                    Key=self._object_key,
                    Body=b'',
                    StorageClass=self._config.storage_class
                )
                self._size = 0
            elif size < self._size:
                # Read, truncate, write
                response = self._client.get_object(
                    Bucket=self._config.bucket,
                    Key=self._object_key
                )
                existing = response['Body'].read()
                
                self._client.put_object(
                    Bucket=self._config.bucket,
                    Key=self._object_key,
                    Body=existing[:size],
                    StorageClass=self._config.storage_class
                )
                self._size = size
            elif size > self._size:
                # Extend with zeros
                response = self._client.get_object(
                    Bucket=self._config.bucket,
                    Key=self._object_key
                )
                existing = response['Body'].read()
                
                new_data = existing + b'\x00' * (size - len(existing))
                self._client.put_object(
                    Bucket=self._config.bucket,
                    Key=self._object_key,
                    Body=new_data,
                    StorageClass=self._config.storage_class
                )
                self._size = size
    
    def delete(self) -> None:
        """Delete the storage object from S3."""
        try:
            self._client.delete_object(
                Bucket=self._config.bucket,
                Key=self._object_key
            )
            self._size = 0
        except ClientError as e:
            raise StorageError(f"Failed to delete object: {e}")
    
    def exists(self) -> bool:
        """Check if storage object exists."""
        try:
            self._client.head_object(
                Bucket=self._config.bucket,
                Key=self._object_key
            )
            return True
        except ClientError:
            return False
    
    # ========================================================================
    # S3-Specific Methods
    # ========================================================================
    
    def get_object_metadata(self) -> Dict:
        """Get S3 object metadata."""
        try:
            response = self._client.head_object(
                Bucket=self._config.bucket,
                Key=self._object_key
            )
            return {
                'size': response['ContentLength'],
                'last_modified': response['LastModified'],
                'etag': response['ETag'],
                'storage_class': response.get('StorageClass', 'STANDARD'),
                'content_type': response.get('ContentType', 'application/octet-stream'),
            }
        except ClientError as e:
            raise StorageError(f"Failed to get object metadata: {e}")
    
    def copy_to(self, dest_key: str) -> None:
        """Copy storage object to another key."""
        try:
            self._client.copy_object(
                Bucket=self._config.bucket,
                CopySource={'Bucket': self._config.bucket, 'Key': self._object_key},
                Key=dest_key,
                StorageClass=self._config.storage_class
            )
        except ClientError as e:
            raise StorageError(f"Failed to copy object: {e}")
    
    @property
    def bucket(self) -> str:
        """Return the S3 bucket name."""
        return self._config.bucket
    
    @property
    def key(self) -> str:
        """Return the S3 object key."""
        return self._object_key
    
    # ========================================================================
    # Context Manager / Cleanup
    # ========================================================================
    
    def __repr__(self) -> str:
        """Return string representation."""
        return (
            f"S3StorageBackend("
            f"bucket='{self._config.bucket}', "
            f"key='{self._object_key}', "
            f"size={self._size})"
        )


__all__ = [
    'S3Config',
    'S3StorageBackend',
    'HAS_BOTO3',
    'HAS_AIOBOTO3',
]
